{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78f726",
   "metadata": {},
   "source": [
    "In this problem, we solve the following ridge regression problem:\n",
    "$$\\min_{x \\in \\mathbb{R}^{n}}f_{\\lambda}(x) \\qquad \\text{where} \\qquad f_{\\lambda}(x) \\doteq \\frac{1}{2}\\left\\{\\frac{1}{m}\\|Ax - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}\\right\\}$$\n",
    "via gradient descent with step size $\\eta > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc801d",
   "metadata": {},
   "source": [
    "# Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_f(A, y, lmbda):\n",
    "    m, n = A.shape\n",
    "    return lambda x: 0.5 * ((np.linalg.norm(A @ x - y) ** 2) / m + lmbda * np.linalg.norm(x) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92713fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_sol(A, y, lmbda):\n",
    "    m, n = A.shape\n",
    "    # Closed-form ridge regression solution to compare to\n",
    "    return np.linalg.inv(A.T @ A + lmbda * m * np.eye(A.shape[1])) @ A.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93272296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_max(A):\n",
    "    S = np.linalg.svd(A, full_matrices=True, compute_uv=False)\n",
    "    return S[0]\n",
    "\n",
    "def sigma_min(A):\n",
    "    S = np.linalg.svd(A, full_matrices=True, compute_uv=False)\n",
    "    if S.shape[0] < A.shape[1]:\n",
    "        return np.float64(0.0)\n",
    "    else:\n",
    "        return S[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbf371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(x0, grad_fn, eta, T):\n",
    "    x_history = []\n",
    "    x = x0\n",
    "    for t in range(T):\n",
    "        x_history.append(copy.deepcopy(x))\n",
    "        x = ...  # TODO: update rule of GD\n",
    "    return np.array(x_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9283e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_opt(A, lmbda):\n",
    "    m, n = A.shape\n",
    "    return 2 * m / (sigma_max(A) ** 2 + sigma_min(A) ** 2 + 2 * lmbda * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gd(A, y, lmbda, eta, T):\n",
    "    m, n = A.shape\n",
    "    def grad_fn(x):\n",
    "        return ...  # TODO: gradient of our ridge objective\n",
    "    x0 = np.random.standard_normal((n,))\n",
    "    return gd(x0, grad_fn, eta, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_sgd(A, y, lmbda, eta, T):\n",
    "    m, n = A.shape\n",
    "    def grad_fn(x):\n",
    "        return ... # TODO: SGD gradient estimate for our ridge objective \n",
    "        # Hint: to generate a random index, you might want to check out np.random.randint\n",
    "    x0 = np.random.standard_normal((n,))\n",
    "    return gd(x0, grad_fn, eta, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_average(x_history):\n",
    "    time_sum = np.cumsum(x_history, axis=0)\n",
    "    time_avg = time_sum / (1 + np.arange(x_history.shape[0]).reshape(-1, 1))\n",
    "    return time_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e682cd",
   "metadata": {},
   "source": [
    "# Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_1d(A, y):\n",
    "    plt.xlabel(\"$a$\")\n",
    "    plt.ylabel(\"$b$\")\n",
    "    plt.plot(A[:, 0], y, marker=\"o\", linestyle=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_2d(A, y):\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    ax.set_xlabel(\"$a_{1}$\")\n",
    "    ax.set_ylabel(\"$a_{2}$\")\n",
    "    ax.plot(A[:, 0], A[:, 1], zs=y, marker=\"o\", linestyle=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves_1d(A, y, lmbdas, x_star):\n",
    "    m, n = A.shape\n",
    "    assert n == 1\n",
    "    for lmbda in lmbdas:\n",
    "        f = make_f(A, y, lmbda)\n",
    "        x_range = np.linspace(start=x_star - 10, stop=x_star + 10, num=1000).reshape(-1, 1)\n",
    "        fx_range = np.array([f(x) for x in x_range])\n",
    "\n",
    "        x_RR = ridge_regression_sol(A, y, lmbda)\n",
    "        fx_RR = f(x_RR)\n",
    "\n",
    "        plt.xlabel(\"$x$\")\n",
    "        plt.ylabel(\"$f_{\\lambda}(x)$\")\n",
    "        plt.plot(x_range, fx_range, label = \"$\\lambda = %s$\"%lmbda)\n",
    "\n",
    "        plt.plot(x_RR, fx_RR, 'o')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_lines_1d(A, y, x_history, x_star, ts, time_avg=False):\n",
    "    plt.xlabel(\"$a$\")\n",
    "    plt.ylabel(\"$b$\")\n",
    "    plt.plot(A[:, 0], y, \".\")\n",
    "    \n",
    "    if time_avg:\n",
    "        x_history = time_average(x_history)\n",
    "\n",
    "    for t in ts:\n",
    "        x_t = x_history[t-1][0]\n",
    "        plt.plot([-1, 1], [-x_t, x_t], label=\"$x_{%s}$\"%t)\n",
    "    plt.plot([-1, 1], [-x_star, x_star], label=\"$x_{\\lambda}^{\\star}$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x_trajectory_1d(f, x_history, x_star, time_avg=False):\n",
    "    if time_avg:\n",
    "        x_history = time_average(x_history)\n",
    "        \n",
    "    x_range = np.linspace(start=x_star - 3, stop=x_star + 3, num=1000).reshape(-1, 1)\n",
    "    fx_range = np.array([f(x) for x in x_range])\n",
    "\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$f_{\\lambda}(x)$\")\n",
    "    plt.plot(x_range, fx_range)\n",
    "    \n",
    "    plt.plot(x_star, f(x_star), 'o')\n",
    "\n",
    "    f_history = np.array([f(x) for x in x_history]).reshape(-1, 1)\n",
    "\n",
    "    plt.quiver(x_history[:-1], f_history[:-1],\\\n",
    "           x_history[1:]-x_history[:-1], f_history[1:]-f_history[:-1],\\\n",
    "           scale_units='xy', angles='xy', scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x_trajectory_2d(f, x_history, x_star, time_avg=False):\n",
    "    if time_avg:\n",
    "        x_history = time_average(x_history)\n",
    "        \n",
    "    plt.xlabel(\"$x_{1}$\")\n",
    "    plt.ylabel(\"$x_{2}$\")\n",
    "    \n",
    "    plt.plot(x_star[0], x_star[1], 'o')\n",
    "    \n",
    "    plt.quiver(x_history[:-1,0], x_history[:-1,1],\\\n",
    "           x_history[1:,0]-x_history[:-1,0], x_history[1:,1]-x_history[:-1,1],\\\n",
    "           scale_units='xy', angles='xy', scale=1)\n",
    "    \n",
    "    xmin, xmax, ymin, ymax = plt.axis()\n",
    "    x_range = np.linspace(start=xmin, stop=xmax, num=100)\n",
    "    y_range = np.linspace(start=ymin, stop=ymax, num=100)\n",
    "    xv, yv = np.meshgrid(x_range, y_range)\n",
    "    z = np.stack((xv, yv), axis=-1)\n",
    "    fz = np.zeros((z.shape[0], z.shape[1]))\n",
    "    for i in range(z.shape[0]):\n",
    "        for j in range(z.shape[1]):\n",
    "            fz[i][j] = f(z[i][j])\n",
    "    min_fz = np.min(fz)\n",
    "    max_fz = np.max(fz)\n",
    "    level1 = min_fz + 0.05 * (max_fz - min_fz)\n",
    "    level2 = min_fz + 0.15 * (max_fz - min_fz)\n",
    "    plt.contour(x_range, y_range, fz, levels=[level1, level2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_x(A, x_history, x_star, time_avg=False):\n",
    "    if time_avg:\n",
    "        x_history = time_average(x_history)\n",
    "        \n",
    "    x_history = x_history.T  # now it's a (N x T) matrix where each column is an x_t\n",
    "    \n",
    "    plt.xlabel(\"$t$\")\n",
    "    plt.ylabel(\"$|x_{t} - x_{\\lambda}^{\\star}|_{i}$\")\n",
    "    plt.yscale(\"log\")\n",
    "    for i in range(A.shape[1]):\n",
    "        dist_xi = np.abs(x_history[i] - x_star[i])\n",
    "        plt.plot(dist_xi, label=\"i=%s\" % (i + 1))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbe728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_z(A, x_history, x_star, time_avg=False):\n",
    "    if time_avg:\n",
    "        x_history = time_average(x_history)\n",
    "        \n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    x_history = x_history.T  # now it's a (N x T) matrix where each column is an x_t\n",
    "    z_history = Vt @ x_history  # project each x_t onto the V basis\n",
    "    z_star = Vt @ x_star\n",
    "    \n",
    "    plt.xlabel(\"$t$\")\n",
    "    plt.ylabel(\"$|v_{i}^{\\\\top}(x_{t} - x_{\\lambda}^{\\star})|$\")\n",
    "    plt.yscale(\"log\")\n",
    "    for i in range(A.shape[1]):\n",
    "        dist_zi = np.abs(z_history[i] - z_star[i])\n",
    "        plt.plot(dist_zi, label=\"i=%s, $\\sigma_{%s} = %s$\" % (i + 1, i + 1, round(S[i], 2)))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a68bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_f(f, x_history, x_star, time_avg=False):\n",
    "    if time_avg:\n",
    "        x_history = time_average(x_history)\n",
    "    \n",
    "    f_history = np.array([f(x_history[i]) for i in range(len(x_history))])\n",
    "    f_star = f(x_star)\n",
    "    \n",
    "    plt.xlabel(\"$t$\")\n",
    "    plt.ylabel(\"$|f_{\\lambda}(x_{t}) - f_{\\lambda}(x_{\\lambda}^{\\star})|$\")\n",
    "    dist_f = np.abs(f_history - f_star)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.plot(dist_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde44cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence_heatmap(A, y, etas, lmbdas, gd_fn, T, convergence_tol, time_avg):\n",
    "    eta_lmbda_matrix = np.zeros(shape=(len(etas), len(lmbdas)))\n",
    "    for i in range(len(etas)):\n",
    "        for j in range(len(lmbdas)):\n",
    "            x_history = gd_fn(A, y, lmbdas[j], etas[i], T)\n",
    "            if time_avg:\n",
    "                x_history = time_average(x_history)\n",
    "            x_T = x_history[-1]\n",
    "            x_rr = ridge_regression_sol(A, y, lmbdas[j])\n",
    "            eta_lmbda_matrix[i, j] = 1.0 if np.linalg.norm(x_rr - x_T) < convergence_tol else 0.0\n",
    "\n",
    "    plt.xlabel(\"$\\eta$\")\n",
    "    plt.ylabel(\"$\\lambda$\")\n",
    "\n",
    "    plt.xticks(range(len(etas)), etas, rotation = 45)\n",
    "    plt.yticks(range(len(lmbdas)), lmbdas)\n",
    "\n",
    "    plt.imshow(eta_lmbda_matrix)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_ticks([0,1])\n",
    "    cbar.set_ticklabels([\"Does not \\n converge\", \"Converges\"])\n",
    "    \n",
    "def convergence_heatmap_gd(A, y, etas, lmbdas, T=200, convergence_tol=0.1, time_avg=False):\n",
    "    convergence_heatmap(A,  y, etas, lmbdas, ridge_regression_gd, T, convergence_tol, time_avg)\n",
    "    \n",
    "def convergence_heatmap_sgd(A, y, etas, lmbdas, T=200, convergence_tol=0.1, time_avg=False):\n",
    "    convergence_heatmap(A,  y, etas, lmbdas, ridge_regression_sgd, T, convergence_tol, time_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652980d",
   "metadata": {},
   "source": [
    "# Part 1: $n = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab649cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1 = 50\n",
    "n_1 = 1\n",
    "noise_std_1 = 0.5\n",
    "x_star_1 = np.array([3]).reshape(n_1,)  # (n,)\n",
    "A_1 = np.random.uniform(low=-1, high=1, size=(m_1, n_1))  # (m, n)\n",
    "y_1 = A_1 @ x_star_1 + noise_std_1 * np.random.standard_normal((m_1, ))  # (m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e030cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to show us what the data looks like\n",
    "plot_data_1d(A_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9172457",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves_1d(A_1, y_1, [0, 0.1, 0.5], x_star_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96edfca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda_1 = 0.1  # TODO: Change this and write your thoughts in the answer PDF\n",
    "eta_1 = 1.1 * eta_opt(A_1, lmbda_1)  # TODO: Change this and write your thoughts in the answer PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52bb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "f_1 = make_f(A_1, y_1, lmbda_1)\n",
    "x_history_1 = ridge_regression_gd(A_1, y_1, lmbda_1, eta_1, T)  # TODO: Replace with SGD and write down your thoughts.\n",
    "x_rr_1 = ridge_regression_sol(A_1, y_1, lmbda_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3779e",
   "metadata": {},
   "source": [
    "Here we visualize the regression lines obtained from our gradient descent iterates compared to the regression line obtained from the closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_lines_1d(A_1, y_1, x_history_1, x_rr_1, [1, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eadf358",
   "metadata": {},
   "source": [
    "Here we visualize the convergence of $x_{t}$ within the standard basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence_x(A_1, x_history_1, x_rr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ea9f1",
   "metadata": {},
   "source": [
    "Here we visualize the convergence of $x_{t}$ within the $V$ basis. Since we are in 1D, the two plots look the same. But they will soon diverge as we move to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5806b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence_z(A_1, x_history_1, x_rr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd7a667",
   "metadata": {},
   "source": [
    "Here we visualize the convergence of $f_{\\lambda}(x_{t})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bab06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence_f(f_1, x_history_1, x_rr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e4ea3",
   "metadata": {},
   "source": [
    "The next plot shows how $x_{t}$ bounces around the loss landscape, more precisely plotting $(x_{t}, f_{\\lambda}(x_{t}))$. Arrows point from $(x_{t}, f_{\\lambda}(x_{t}))$ to $(x_{t + 1}, f_{\\lambda}(x_{t + 1}))$ for all $t$. The optimal point $(x_{\\lambda}^{\\star}, f_{\\lambda}(x_{\\lambda}^{\\star})$ is highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_trajectory_1d(f_1, x_history_1, x_rr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f956a",
   "metadata": {},
   "source": [
    "We now plot the convergence of gradient descent for different values of $\\eta$ and $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [1e-3, 2*1e-3, 5*1e-3, 1e-2, 2*1e-2, 5*1e-2, 1e-1, 2*1e-1, 5*1e-1, 1] \n",
    "lmbdas = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "convergence_heatmap_gd(A_1, y_1, etas, lmbdas, T = 100, convergence_tol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9e23",
   "metadata": {},
   "source": [
    "# Part 2: $n = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0449dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_2 = 50\n",
    "n_2 = 2\n",
    "noise_std_2 = 0.5\n",
    "x_star_2 = np.array([3, 1]).reshape(n_2,)  # (n,)\n",
    "A_2 = np.random.uniform(low=-1, high=1, size=(m_2, n_2))  # (m, n)\n",
    "y_2 = A_2 @ x_star_2 + noise_std_2 * np.random.standard_normal((m_2, ))  # (m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14703d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to show us what the data looks like\n",
    "plot_data_2d(A_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eacade",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda_2 = 0.0  # TODO: Change this and write your thoughts in the answer PDF\n",
    "eta_2 = 1.5 * eta_opt(A_2, lmbda_2)  # TODO: Change this and write your thoughts in the answer PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "f_2 = make_f(A_2, y_2, lmbda_2)\n",
    "x_history_2 = ridge_regression_gd(A_2, y_2, lmbda_2, eta_2, T)  # TODO: Replace with SGD and write down your thoughts.\n",
    "x_rr_2 = ridge_regression_sol(A_2, y_2, lmbda_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea4299",
   "metadata": {},
   "source": [
    "Here we visualize the convergence of $x_{t}$ within the standard basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025557b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_convergence_x(A_2, x_history_2, x_rr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663622d",
   "metadata": {},
   "source": [
    "Here we visualize the convergence of $x_{t}$ within the $V$ basis. Note that while distance curves in the above figure might be non-monotonic or irregular, the below figure curves will be monotonic decreasing. (Why? Look at the equations in the PDF.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8abadfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_convergence_z(A_2, x_history_2, x_rr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceffc487",
   "metadata": {},
   "source": [
    "Here we plot the convergence of $f_{\\lambda}(x_{t})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac54d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_convergence_f(f_2, x_history_2, x_rr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbe7a6",
   "metadata": {},
   "source": [
    "We now show how $x_{t}$ bounces around $\\mathbb{R}^{2}$. Arrows point from $x_{t}$ to $x_{t + 1}$. The optimal point $(x_{\\lambda}^{\\star}, f_{\\lambda}(x_{\\lambda}^{\\star}))$ is highlighted, as well as two level sets of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe583a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_trajectory_2d(f_2, x_history_2, x_rr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32133a48",
   "metadata": {},
   "source": [
    "We now plot the convergence of gradient descent for different values of $\\eta$ and $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [1e-3, 2*1e-3, 5*1e-3, 1e-2, 2*1e-2, 5*1e-2, 1e-1, 2*1e-1, 5*1e-1, 1] \n",
    "lmbdas = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "convergence_heatmap_gd(A_2, y_2, etas, lmbdas, T = 100, convergence_tol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460f315",
   "metadata": {},
   "source": [
    "# Part 3: $n \\gg 2$\n",
    "\n",
    "Here, we cannot use our visual intuition to see the trajectory of $x_{t}$. But we can use the principles we derived to observe convergence anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_10 = 50\n",
    "n_10 = 10\n",
    "noise_std_10 = 0.5\n",
    "x_star_10 = np.random.uniform(low=1, high=3, size=(n_10,))  # (n,)\n",
    "A_10 = np.random.uniform(low=-1, high=1, size=(m_10, n_10))  # (m, n)\n",
    "y_10 = A_10 @ x_star_10 + noise_std_10 * np.random.standard_normal((m_10, ))  # (m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24006459",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda_10 = 0.01  # TODO: Change this and write your thoughts in the answer PDF\n",
    "eta_10 = 1.1 * eta_opt(A_10, lmbda_10)  # TODO: Change this and write your thoughts in the answer PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76adf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "f_10 = make_f(A_10, y_10, lmbda_10)\n",
    "x_history_10 = ridge_regression_gd(A_10, y_10, lmbda_10, eta_10, T)  # TODO: Replace with SGD and write down your thoughts.\n",
    "x_rr_10 = ridge_regression_sol(A_10, y_10, lmbda_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac74497a",
   "metadata": {},
   "source": [
    "Here we visualize the convergence of $x_{t}$ within the standard basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db822a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_convergence_x(A_10, x_history_10, x_rr_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668c2bc",
   "metadata": {},
   "source": [
    "Here we visualize the convergence of $x_{t}$ within the $V$ basis. Note that the below figure curves will be monotonic decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651afc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_convergence_z(A_10, x_history_10, x_rr_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f9ddd",
   "metadata": {},
   "source": [
    "Here we plot the convergence of $f_{\\lambda}(x_{t})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9049160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_convergence_f(f_10, x_history_10, x_rr_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fd899",
   "metadata": {},
   "source": [
    "We now plot the convergence of gradient descent for different values of $\\eta$ and $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [1e-3, 2*1e-3, 5*1e-3, 1e-2, 2*1e-2, 5*1e-2, 1e-1, 2*1e-1, 5*1e-1, 1] \n",
    "lmbdas = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "convergence_heatmap_gd(A_10, y_10, etas, lmbdas, T = 5000, convergence_tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea1e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ea918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
